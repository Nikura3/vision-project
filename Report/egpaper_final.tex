\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{alphabeta}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\graphicspath{ {./images/} }

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Cycle Generative Adversarial Nets for Style Transfer of Monet Paintings}

\author{Solmaz Mohammadi\\
{\tt\small solmaz.mohammadi@studenti.unipd.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Nicla Faccioli\\
{\tt\small nicla.faccioli@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This article aims at learning the style of the impressionist
painter, Claude Monet. Since the generative adversarial
model has its limitations when it comes to more complex
computer vision problems, we will be using a variation
of generative adversarial models, cycle consistent GAN to capture the
“impression” of an artist from their surroundings. CycleGAN is an unpaired image to image translation using cycle consistent adversarial nets.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The focus of this paper is to reproduce the style of Claude Monet, the
influential impressionist painter in the nineteenth century. Claude Monet’s style is unique and recognizable by his short and thick strokes that capture the essence of the subject. Monet’s work is particularly interesting because of the strong logarithmic correlations within his paintings that makes it possible to generate new Monet-like images. \cite{monetlog}
This project attempts to transfer the painter's style into existing real-life photos. Style transfer is a computer vision technique that aims to combine two images together. In particular, given a content image (i.e. is interesting for its content) and a style image (i.e. is interesting for its style), the task is to blend them together in order to obtain as output a third image which will represent the content of the first image in the style of the second one. Therefore, we aim to learn a mapping function G: X$\longrightarrow$Y such that the distribution of G(X) is inseparable from the disribution of Y using the adversarial loss (i.e. the discriminator). To further constraint the generator, we pair it with an inverse map F:Y$\longrightarrow$X and use cycle consistency loss to enforce F(G(X))$\sim$X. \cite{cyclegan}
%\\ results


\section{Related Work}
In \textbf{Generative Adversarial Nets (GANs)}, two models are trained simultaneously: a generative model G the captures the data distribution, and a discriminative model D that estimates the probability that a sample is from the original data rather than generated by G \cite{simplegan}. The training procedure for G is to maximize the probability of D making a mistake. Therefore, it corresponds to a 2-player minimax problem in which generator tries to “trick” the discriminator. This framework is then the core idea of many generative model applications including cycle consistent GANs. However, the simple GAN is under-constrained for style transfer problem \cite{cyclegan}. We apply a cycle consistency loss objective to further condition GAN.  

 \textbf{Paired Image-to-Image Translation}: In some applications the aim is not to generate the image from scratch but rather, “translate” an image from a domain X, to another domain Y.  This can be done by conditioning GAN to learn a dataset of paired images and the function "translating" them. This approach introduces cGAN, where the given the input, the output is conditoned by a set of constraints. The conditional GANs learn a mapping from observed data x and random noise vector z, to the output y, G:{x,z}$\rightarrow$y \cite{imgtoimg}.
\\ cGANs in the context of image translation can be generalized to cover various problems. Image prediction from a map \cite{example1}, future frame prediction \cite{example2}, and image generation from sparse annotations are some of the examples \cite{example3}. Image-to-Image mappings are also the core idea for style transfer problem, but with GAN applied unconditionally \cite{example4}.\\

\section{Dataset}

The dataset was collected from Kaggle, a platform in which competitions and challenges are proposed and where we got the idea for this project. The dataset is composed by:
\begin{itemize}
\item A set of 300 Monet paintings sized 256x256 in both JPEG and TFRecord format;
\item A set of 7028 photos sized 256x256 in both JPEG and TFRecord format.
\end{itemize}
We used the Monet paintings to train our model so that it could learn Monet style and then we used the photos to apply the learned style. 
% data preprocessing

\section{Method}
%discuss your approach for solving the problems that you set up in the introduction. Why is your approach the right thing to do? Did you consider alternative approaches? It may be helpful to include figures, diagrams, or tables to describe your method or compare it with others.
In our problem given the real-life images $x_{i}\in X$ and monet paintings $x_{i}\in Y$ we aim to learn mapping functions between the two domains X and Y, donating the data distribution as $x \sim p_{data}(x)$ and $y \sim p_{data}(y)$. Our model captures the mapping between domains using two conditionally generative \cite{imgtoimg} models G : X $\longrightarrow$ Y and F : Y $\longrightarrow$ X. Additionally, we also include two adversarial discriminators $D_{X}$ and $D_{Y}$ where task of $D_{X}$ is to discriminate between \{x\} and translated images \{F(y)\}. Similarly, $D_{Y}$ aims to distinguish between \{y\} and \{G(x)\} \cite{cyclegan}. The objective of our work is to minimize two losses: \emph{adversarial loss} \cite{simplegan} which helps to close the gap between the data distribution of target domain and the generated images; and \emph{cycle consistency loss} to keep the results of the two mapping functions consistent.

\subsection{Adversarial Loss}
To learn the distribution of $x \sim p_{data}(x)$, we first define a prior on input noise variables $p_{z}(z)$ and define a mapping G:z $\longrightarrow$ that can be trained using a convolutional neural network \cite{cnn}. We also define a discriminator D (also trained using a CNN) that instead of a single scalar \cite{simplegan}, outputs an image with decreased dimentionality where pixels that are more likely to be in data distribution $p_{data}(x)$ have higher values. We train both models simultaneously in a two-player minimax approach that can be expressed as:
[formula1]
The adversarial loss, maximizes the probablity of discriminator D differentiating training examples and generated samples
\subsection{Cycle Consistency Loss}
\subsection{Full Objective}
\subsection{Implementation}

\section{Experiments}
In order to evaluate the results of our work we looked for some metrics that could help us in this task \cite{metrics}. \\
\textbf{Fréchet Inception Distance (FID)} Introduced by \textit{Heusel et al.} \cite{fid}, FID measures the distance between the feature vector of the generated images and the feature vector of the real images. Therefore, a lower FID score represents a better performance of the generator since the images it creates are similar to the real ones. The formula to calculate the FID score is the following:
$$ \textstyle FID(r, g) = \| \mu_{r} - \mu_{g} \| _{2}^{2} + Tr(\sum_{r} +  \sum_{g} - 2(\sum_{r}\sum_{g})^\frac{1}{2}) $$
Where $\textstyle (\mu_{r} , \sum_{r})$ is the multivariate normal distribution estimated from a specific layer of Inception Net features for real images and $\textstyle (\mu_{g} , \sum_{g})$ is the multivariate normal distribution estimated from a specific layer of Inception Net features for generated images.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
